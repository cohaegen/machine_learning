{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Pre-trained Transformer (GPT) in Tensorflow\n",
    "\n",
    "This is a fun experiment in GPTs inspired by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY. He's a great teacher and has taught classes, written about ML, and posted videos about ML for years.\n",
    "\n",
    "Karpathy walks through writing a GPT from scratch using the mini-Shakespeare dataset and PyTorch. I'm more used to TensorFlow, so I wanted to try and follow along using TF. I didn't quite write this from scratch; Andrej implements a multi-head attention layer from scratch, but TensorFlow includes a MultiHeadAttention layer so I used that. It's worth paying attention in Andrej's tutorial to see what's going on in an attention layer though; the key, value, and query concepts are useful to understand.\n",
    "\n",
    "I also wanted to try more data, so I used the complete works of William Shakespeare instead of the mini-Shakespeare dataset.\n",
    "\n",
    "I put a slight spin on the training/inference approach by creating \"training\" and \"production\" models. The training model requires tokens as input. The tokens have been translated from the raw input bytes. As output, it generates logits for each of the possible tokens that it thinks will come next. The production model takes characters in and spits characters out. It does this by adding an encoding and decoding layer to the inference model after training. Encoding translates the raw bytes into a smaller vocabulary of tokens using a tf.keras.layers.StringLookup layer. On the output, it chooses a next token given the logits and then decodes back to characters. I think this makes it a little easier to pass in data: all you need to do on the input side is split up a string into characters and truncate or pad to the right length, and on the output side, you get characters right out of the model.\n",
    "\n",
    "The original paper on Transformers (\"Attention is All You Need\", Vaswani et al.) is worth a read: https://arxiv.org/pdf/1706.03762.pdf. This implements just the \"decoder\" portion on the right. The decoder predicts the next character in a sequence given a string of recent characters (e.g. given \"To be or not \", it might fill in \"t\", \"o\", \" \", \"b\", \"e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of a Generative Pre-Trained Transformer (GPT) in Tensorflow\n",
    "\"\"\"\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define some constants\n",
    "INPUT_FILE = 't8.shakespeare.txt'\n",
    "TIMESERIES_CONTEXT = 32  # how many characters to use for prediction?\n",
    "BATCH_SIZE = 32\n",
    "NUM_HEADS = 4  # Number of attention heads in each layer\n",
    "HEAD_SIZE = 32  # Number of units in each head\n",
    "DROPOUT = 0.2  # Dropout regularization in the attention and dense layers\n",
    "NUM_LAYERS = 6  # Number of attention layers\n",
    "LEARNING_RATE = 3e-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers need some sort of information that associates each input with its position. I'm doing that here with just a range of integers fed into an embedding layer. That gets added to an embedding layer that represents the input characters themselves. This layer class implements both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, context_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.tok_embedding = tf.keras.layers.Embedding(context_size, embed_size)\n",
    "        self.positions = tf.range(0, context_size)\n",
    "    \n",
    "    def call(self, values):\n",
    "        return self.pos_embedding(values) + self.tok_embedding(self.positions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the actual Transformer Decoder layer. This is based on the Attention Is All You Need paper but with the layer normalization performed before attention and feed-forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads: int, head_size: int, dropout: int):\n",
    "        super().__init__()\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size, dropout=dropout)\n",
    "        # The feed-forward layer includes a multiple of the input size in a non-linear multi-layer perceptron as a calculation stage,\n",
    "        # followed by a linear layer that maps the dimensions back to the input size\n",
    "        self.feed_forward = tf.keras.Sequential([tf.keras.layers.Dense(4*num_heads*head_size, activation='gelu'),\n",
    "                                                 tf.keras.layers.Dense(num_heads*head_size),\n",
    "                                                 tf.keras.layers.Dropout(dropout)])\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, values):\n",
    "        norm_values = self.layer_norm1(values)\n",
    "        # At both the attention and feed-forward layers, we include a residual (values and attn) that helps accelerate convergence\n",
    "        attn = values + self.attention(norm_values, norm_values, attention_mask=np.tri(TIMESERIES_CONTEXT))\n",
    "        norm_attn = self.layer_norm2(attn)\n",
    "        feed_fwd = attn + self.feed_forward(norm_attn)\n",
    "        # The output shape is going to be the same as the input shape\n",
    "        return feed_fwd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some utility functions for reading data and generating random slices for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_filename: str):\n",
    "    \"\"\"Read the input file and return the encoded dataset and an encoding layer\"\"\"\n",
    "    raw_input = list(open(input_filename).read())\n",
    "    vocabulary = sorted(list(set(raw_input)))\n",
    "    encoder = tf.keras.layers.StringLookup(vocabulary=vocabulary)\n",
    "    enc_input = encoder(raw_input).numpy()\n",
    "    return enc_input, encoder\n",
    "\n",
    "\n",
    "def generate_batches(data: np.array, context_size, batch_size):\n",
    "    \"\"\"Generate batches of value,target pairs for training\"\"\"\n",
    "    # Create shuffled starting offsets into the data\n",
    "    starting_offsets = np.arange(len(data)-context_size)\n",
    "    np.random.shuffle(starting_offsets)\n",
    "    for batch_idx in range(0, len(starting_offsets), batch_size):\n",
    "        batch_starting_offsets = starting_offsets[batch_idx:(batch_idx+batch_size)].reshape((-1,1))\n",
    "        # Turn the starting offsets into indices: this will project the arange across all of the \n",
    "        # starting_offsets so something like [[5], [11], [3]] becomes [[5, 6, 7], [11, 12, 13], [3, 4, 5]]\n",
    "        indices = batch_starting_offsets + np.arange(context_size)\n",
    "        values = data[indices]\n",
    "        # The targets (what we want to predict) are just the next characters\n",
    "        targets = data[indices+1]\n",
    "        yield values, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(encoder):\n",
    "    \"\"\"Create the GPT model and return it\"\"\"\n",
    "    # At each layer, I'll show the output dimensions. B means the batch dimension, T means the time dimension (characters), and C is the channel dimension (number of different values for each character)\n",
    "    values_input = tf.keras.layers.Input(shape=(TIMESERIES_CONTEXT,), name='values_input')  # (B, T)\n",
    "    layer = TokenAndPositionEmbedding(encoder.vocabulary_size(), TIMESERIES_CONTEXT, NUM_HEADS*HEAD_SIZE)(values_input)  # (B, T, NUM_HEADS*HEAD_SIZE)\n",
    "    for _ in range(NUM_LAYERS):\n",
    "        layer = TransformerDecoder(NUM_HEADS, HEAD_SIZE, DROPOUT)(layer)  # (B, T, NUM_HEADS*HEAD_SIZE)\n",
    "    # One last layer normalization\n",
    "    layer = tf.keras.layers.LayerNormalization()(layer)  # (B, T, NUM_HEADS*HEAD_SIZE)\n",
    "    # And map to the vocabulary size\n",
    "    output = tf.keras.layers.Dense(encoder.vocabulary_size())(layer)  # (B, T, C)\n",
    "\n",
    "    model = tf.keras.Model(inputs=values_input, outputs=output)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # This loss lets us pass in integers rather than having to one-hot encode\n",
    "    optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all the pieces together: create the model, train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 20:21:02.268717: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-06-23 20:21:02.268748: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-06-23 20:21:02.268754: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-06-23 20:21:02.268833: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-06-23 20:21:02.269070: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " values_input (InputLayer)   [(None, 32)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 32, 128)           15872     \n",
      " ng (TokenAndPositionEmbedd                                      \n",
      " ing)                                                            \n",
      "                                                                 \n",
      " transformer_decoder (Trans  (None, 32, 128)           198272    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder_1 (Tra  (None, 32, 128)           198272    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_2 (Tra  (None, 32, 128)           198272    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_3 (Tra  (None, 32, 128)           198272    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_4 (Tra  (None, 32, 128)           198272    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " transformer_decoder_5 (Tra  (None, 32, 128)           198272    \n",
      " nsformerDecoder)                                                \n",
      "                                                                 \n",
      " layer_normalization_12 (La  (None, 32, 128)           256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 32, 92)            11868     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1217628 (4.64 MB)\n",
      "Trainable params: 1217628 (4.64 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 20:21:13.657784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 170568/Unknown - 20824s 122ms/step - loss: 1.3705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 02:08:15.844809: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 9877130478973186312\n",
      "2023-06-24 02:08:15.844956: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 13637574184222110462\n",
      "2023-06-24 02:08:15.844961: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6687076510872634115\n",
      "2023-06-24 02:08:15.844972: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 418133863264850504\n",
      "2023-06-24 02:08:15.844976: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16877465800649933919\n",
      "2023-06-24 02:08:15.844985: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7090349029318849798\n",
      "2023-06-24 02:08:15.844990: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17823002550336301023\n",
      "2023-06-24 02:08:15.844994: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8933173654403049148\n",
      "2023-06-24 02:08:15.845003: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8917449608832125185\n",
      "2023-06-24 02:08:15.845007: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 9041400685808813970\n",
      "2023-06-24 02:08:15.845011: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10748251365342915079\n",
      "2023-06-24 02:08:15.845015: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 321315067440150552\n",
      "2023-06-24 02:08:15.845020: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10205511927851969249\n",
      "2023-06-24 02:08:15.845024: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16186058977065806152\n",
      "2023-06-24 02:08:15.845028: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8218348682243451359\n",
      "2023-06-24 02:08:15.845033: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5980399405772901777\n",
      "2023-06-24 02:08:15.845037: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16731947626095831716\n",
      "2023-06-24 02:08:15.845041: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4981326995357748871\n",
      "2023-06-24 02:08:15.845045: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7977283600486230816\n",
      "2023-06-24 02:08:15.845049: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 6881307284218813894\n",
      "2023-06-24 02:08:15.845051: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 9961672466936859695\n",
      "2023-06-24 02:08:15.845053: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16357519980805492332\n",
      "2023-06-24 02:08:15.845056: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 3181044349731033313\n",
      "2023-06-24 02:08:15.845058: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 11718157936823917609\n",
      "2023-06-24 02:08:15.845062: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 168586124488249725\n",
      "2023-06-24 02:08:15.845066: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8091663803323617665\n",
      "2023-06-24 02:08:15.845172: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4137420611841823577\n",
      "2023-06-24 02:08:15.845176: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 5445035330606544423\n",
      "2023-06-24 02:08:15.845187: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4345336175933847481\n",
      "2023-06-24 02:08:15.845191: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4368102349012129947\n",
      "2023-06-24 02:08:15.845199: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1542149422132141025\n",
      "2023-06-24 02:08:15.845205: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15862907307411782413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170568/170568 [==============================] - 20824s 122ms/step - loss: 1.3705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29f48d8a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, encoder = read_input(INPUT_FILE)\n",
    "model = create_model(encoder)\n",
    "model.summary()\n",
    "# Loss should get down to about 1.3\n",
    "# You can pass steps_per_epoch to limit the training to fewer batches. You can get some interesting\n",
    "# results at 5,000 and it takes a lot less time (loss about 1.9). The whole file is about 170k.\n",
    "model.fit(generate_batches(data, TIMESERIES_CONTEXT, BATCH_SIZE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a production model that has a character encoder layer on the input, and a decoder layer on the output that translates logits back into characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseLookupLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, encoder: tf.keras.layers.StringLookup):\n",
    "        super().__init__()\n",
    "        self.decoder = tf.keras.layers.StringLookup(vocabulary=encoder.get_vocabulary(), invert=True)\n",
    "    \n",
    "    def call(self, logits):\n",
    "        # Pick the last character prediction in each batch row\n",
    "        # Then, for each batch row, choose a character based on the logit probabilities\n",
    "        choose_chars = tf.random.categorical(logits[:,-1,:], num_samples=1)\n",
    "        # Decode back to characters\n",
    "        return self.decoder(choose_chars)\n",
    "\n",
    "\n",
    "def create_production_model(model, encoder):\n",
    "    \"\"\"Add a character encoder and decoder layer at the front and back of the model\"\"\"\n",
    "    chars_input = tf.keras.layers.Input(shape=(TIMESERIES_CONTEXT,), dtype=tf.string)\n",
    "    # Strip the input layer off the training model and add our input with an encoder instead\n",
    "    prod_model = tf.keras.Sequential([chars_input,\n",
    "                                      encoder] +\n",
    "                                      model.layers[1:] +\n",
    "                                      [InverseLookupLayer(encoder)])\n",
    "    return prod_model\n",
    "\n",
    "\n",
    "def generate_text(model: tf.keras.Model, context: str, num_chars: int):\n",
    "    print(context, end='')\n",
    "    for _ in range(num_chars):\n",
    "        context = context[-TIMESERIES_CONTEXT:]  # truncate\n",
    "        context = ['[UNK]'] * (TIMESERIES_CONTEXT - len(context)) + list(context)  # pad with \"unknown\" strings at the front\n",
    "        char = model.predict([context], verbose=0)[0,0].decode()\n",
    "        print(char, end='', flush=True)\n",
    "        context += char"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put everything together and generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-24 02:08:17.051930: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " E\n",
      "  CAAUSTER. What say, my mother?\n",
      "  AUSTRIA. How now goes out? subject come to me; and we'll make some christom. Go thou, do you shallow thee, in a hand\n",
      "    And in the night, though ever to reign him\n",
      "    And stand to bed and rise, foreign triumph les\n",
      "    eat me of letters' ohfice. Let's shrive I was so heaven, this is a stattendar. So much the very heart\n",
      "    Of my desire dead meretitude in two weather dreadfully\n",
      "    And smooth far never shaken companion to the Duke.\n",
      "  IACHIMO.                     Fly the city to the DUCHESS OF JOHN WOLSEY\n",
      "\n",
      " FIRST LORD. Fare you well; but how I have heard you meet that he was,\n",
      "    Make it so her since, and almost wounded; with when\n",
      "         the rascal snare, of sea, and so quiet,\n",
      "    Or thou com'st, as thou art the asunders stays into the Worcester's gibe?\n",
      "  MACBETH. Fear not, no, ho! I was a word\n",
      "    deadly knave bears his wifebut fool,\n",
      "    Some witness of your death of incensures?\n",
      "    By his nose good finger him in him\n",
      "    To make a reseeming city"
     ]
    }
   ],
   "source": [
    "prod_model = create_production_model(model, encoder)\n",
    "generate_text(prod_model, context='   A', num_chars=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
